%pyspark
df = sqlContext.sql("select * from salaries")
df_sal = df.select(df.emp_no, df.salary, df.from_date, date_sub(df.to_date , 1).alias("to_date"))
df_sal.write.format("orc").saveAsTable("salaries_new")

df_emp = sqlContext.sql("select t1.emp_no, t1.birth_date, t1.first_name, t1.last_name, t1.gender, t2.hire_date_new from employees t1
join (select emp_no, min(from_date) as hire_date_new from salaries_new group by emp_no) t2 on t1.emp_no = t2.emp_no")

df_emp.write.format("orc").saveAsTable("employees_new")

sqlContext.sql("select * from employees E join (select emp_no from salaries where from_date like '1985-05-%'
and datediff(to_date, from_date) < 15) S on E.emp_no = s.emp_no").show()


# The Data Frame way to solve Task 2

%pyspark
from pyspark.sql import functions as f

# cleansing to_date by one day
df_sal = sqlContext.sql("select * from salaries")
df_sal_new = df_sal.select(df_sal.emp_no, df_sal.salary, df_sal.from_date, date_sub(df_sal.to_date , 1).alias("to_date"))

# cleansing hire_date
df_sal_tmp = df_sal_new.groupBy('emp_no').agg(f.min('from_date').alias('hire_date'))
df_emp = sqlContext.sql("select * from employees")
df_emp_new = df_emp.join(df_sal_tmp, df_emp.emp_no == df_sal_tmp.emp_no, "inner")\
    .select(df_emp.emp_no,df_emp.birth_date, df_emp.first_name, df_emp.last_name, df_emp.gender, df_sal_tmp.hire_date  )

# determine employee lasted less than two weeks in the job in May 1985
df_emp_198505 = sqlContext.sql("select emp_no from salaries where from_date like '1985-05-%' and datediff(to_date, from_date) < 15")
df_emp.join(df_emp_198505, df_emp.emp_no == df_emp_198505.emp_no, "inner")\
    .select(df_emp.emp_no, df_emp.first_name, df_emp.last_name).show()

# save cleansing data into Hive with ORC format
df_sal_new.write.format("orc").saveAsTable("salaries_new")
df_emp_new.write.format("orc").saveAsTable("employees_new")
